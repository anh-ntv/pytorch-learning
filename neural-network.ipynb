{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"neural-network.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOqCJMdGH2DsiqVpYu23tla"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Ddrn46IBbWvJ","colab_type":"text"},"source":["#Neural network model by Pytorh  \n","1. Numpy code: Create basic NN model step by step from init model to compute gradient and train model. This code use Pytorch to enable using GPU\n","\n","2. Pytorch code: Creat basic NN model using pytorch API instead. Code faster, shorter\n"]},{"cell_type":"code","metadata":{"id":"zMgv9rFmn9O9","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592542097652,"user_tz":-420,"elapsed":3919,"user":{"displayName":"Van Anh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjY6DXrvLxeXSifvLvjs3m3nEAebAooQkVA9TUh=s64","userId":"08565157803240051730"}}},"source":["import torch\n","\n","device = torch.device(\"cuda:0\")\n","dtype=torch.float"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EIiuB3PM5CNQ","colab_type":"text"},"source":["##1. Numpy code (but using pytorch instead)"]},{"cell_type":"markdown","metadata":{"id":"N4D3AL0Nah-X","colab_type":"text"},"source":["Check data type"]},{"cell_type":"code","metadata":{"id":"PP7GL2Guq_g_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1592533920504,"user_tz":-420,"elapsed":1364,"user":{"displayName":"Van Anh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjY6DXrvLxeXSifvLvjs3m3nEAebAooQkVA9TUh=s64","userId":"08565157803240051730"}},"outputId":"d83a35ef-826f-4d82-a8fa-975086a3806e"},"source":["a = torch.randint(0, 10, [3, 4], device=device, dtype=torch.float)\n","print(a)\n","# print(a.shape)\n","# w = torch.tensor([10, 10, 10], device=device, dtype=torch.float)\n","w = torch.randint(0, 10, [3, 4], device=device, dtype=torch.float)\n","print(\"W before\", w)\n","# print(\"W shape\", w.shape)\n","# w = w.t()\n","# print(\"W after\", w)\n","# a = a.matmul(w)\n","# print(\"a after mm\", a)\n","# print(a.shape)\n","print(a*w)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[2., 4., 5., 4.],\n","        [7., 1., 1., 7.],\n","        [6., 5., 5., 4.]], device='cuda:0')\n","W before tensor([[4., 3., 9., 4.],\n","        [6., 3., 2., 5.],\n","        [4., 6., 2., 2.]], device='cuda:0')\n","tensor([[ 8., 12., 45., 16.],\n","        [42.,  3.,  2., 35.],\n","        [24., 30., 10.,  8.]], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iwuMd9ccLoTx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1592533941731,"user_tz":-420,"elapsed":2158,"user":{"displayName":"Van Anh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjY6DXrvLxeXSifvLvjs3m3nEAebAooQkVA9TUh=s64","userId":"08565157803240051730"}},"outputId":"6773ef12-6754-4715-804e-320bad29b272"},"source":["x = torch.tensor([1,2,3,4], device=device)\n","y = torch.tensor([3,4,5], device=device)\n","x2, y2 = torch.meshgrid(x, x)\n","print(x2 * torch.eye(4, device=device))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[1., 0., 0., 0.],\n","        [0., 2., 0., 0.],\n","        [0., 0., 3., 0.],\n","        [0., 0., 0., 4.]], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BNjO-Vv_3H43","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592542107915,"user_tz":-420,"elapsed":752,"user":{"displayName":"Van Anh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjY6DXrvLxeXSifvLvjs3m3nEAebAooQkVA9TUh=s64","userId":"08565157803240051730"}}},"source":["def init_model(input_shape=1000, layer_config=[1000, 500, 10], device=\"cuda:0\", dtype=torch.float):\n","  # Init parameter of model\n","  # Return params = [[w1, b1], [w2, b2], ...]\n","  params = []\n","  i_shape = input_shape\n","  for num_n in layer_config:\n","    w = torch.randn([i_shape, num_n], device=device, dtype=dtype)\n","    b = torch.randn([1, num_n], device=device, dtype=dtype)\n","    # w = torch.randint(-10, 10, [i_shape, num_n], device=device, dtype=dtype)\n","    # b = torch.randint(-10, 10, [1, num_n], device=device, dtype=dtype)\n","    i_shape = num_n\n","\n","    params.append([w, b])\n","  return params"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"UzasxlkqqZjU","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592542291335,"user_tz":-420,"elapsed":700,"user":{"displayName":"Van Anh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjY6DXrvLxeXSifvLvjs3m3nEAebAooQkVA9TUh=s64","userId":"08565157803240051730"}}},"source":["def run_model(x=None, model_params=None, acti_fs=['relu', 'softmax']):\n","  # forward phase\n","  # return output of each layer and each step in layer\n","  # output = [[input], [h1, h1_ac], [h2, h2_ac], ...]\n","  out_l = x\n","  assert len(model_params) == len(acti_fs)\n","  outputs = [[x]]\n","  for (w, b), ac in zip(model_params, acti_fs):\n","    out_l_list = []\n","    out_l = out_l.matmul(w) + b\n","    out_l_list.append(out_l)\n","    if ac is 'relu':\n","      out_l = out_l.clamp(min=0)\n","    elif ac is 'softmax':\n","      out_l = torch.nn.Softmax()(out_l)\n","    out_l_list.append(out_l)\n","    outputs.append(out_l_list)\n","  return outputs\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"vESzuDGwUvlW","colab_type":"code","colab":{}},"source":["def loss_fn(y_pred, y_gt):\n","  # compute loss and loss gradient\n","  loss = (y_pred - y_gt).pow(2).sum().item()\n","  loss_grad = 2.0 * (y_pred - y_gt)\n","  return loss, loss_grad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2TCdaDrEUUzC","colab_type":"code","colab":{}},"source":["def backward(model_params=None, acti_fs=['relu', 'softmax'], output_layers=[], lr=0.001, loss_gra=[0.0], device='cuda:0', dtype=torch.float):\n","  ## compute gradient of each layer and update weight, bias\n","  debug = False\n","  grads = []\n","  grad_previous_layer = loss_gra\n","  if debug:\n","    print(\"Step 0\")\n","    print(\"\\tgrad_previous_layer\", grad_previous_layer.shape)\n","  # compute gradient from the last layer to first layer\n","  for idx in range(len(model_params))[::-1]:\n","    w, b = model_params[idx] # w, b of layer\n","    ac =  acti_fs[idx]        # activation func of layer\n","    out_l = output_layers[idx + 1]  # output of layer (because output[0] is input), out_l = [h, h_ac]\n","    out_pre = output_layers[idx]    # output of previous layer (because output[0] is input)\n","\n","    # out_l[0] = w * out_pre[-1] + b\n","    # out_l[1] = ac(out_l[0])\n","    if debug:\n","      print(\"Step\", idx)\n","    if ac == 'softmax':\n","      grad_softmax = out_l[-1] * (1.0 - out_l[-1] + grad_previous_layer / 2.0) # 1x2\n","      grad_softmax = grad_softmax.t().matmul(torch.ones([1, grad_softmax.shape[-1]], device=device, dtype=dtype))\n","      eye_matrix = torch.eye(grad_softmax.shape[-1], device=device, dtype=dtype)\n","      grad_softmax = grad_softmax * eye_matrix  # = 0 for i#j\n","      grad_previous_layer = grad_previous_layer.matmul(grad_softmax)\n","\n","      if debug:\n","        print(\"\\tgrad_softmax\", grad_softmax.shape)\n","        print(\"\\tgrad_previous_layer\", grad_previous_layer.shape)\n","    elif ac == 'relu':\n","      grad_previous_layer[out_l[0] < 0] = 0\n","      if debug:\n","        print(\"\\tgrad_relu/grad_previous_layer\", grad_previous_layer.shape)\n","    if debug:\n","      print(\"\\tout_pre\", out_pre[-1].shape)\n","\n","    grad_w = out_pre[-1].t().mm(grad_previous_layer)\n","    grad_b = grad_previous_layer.clone()\n","    grads.append([grad_w, grad_b])\n","\n","    if debug:\n","      print(\"\\tgrad_w\", grad_w.shape)\n","      print(\"\\tgrad_b\", grad_b.shape)\n","      print(\"\\tw\", w.shape)\n","\n","    grad_previous_layer = grad_previous_layer.mm(w.t())\n","    if debug:\n","      print(\"\\tgrad_previous_layer\", grad_previous_layer.shape)\n","\n","  grads = grads[::-1]\n","  # update weight and bias\n","  if debug:\n","    print(len(model_params), len(grads))\n","  updated_params = []\n","  for (w, b), (grad_w, grad_b) in zip(model_params, grads):\n","    w -= lr * grad_w\n","    b -= lr * grad_b\n","    # print(\"\\tgrad_w\", grad_w)\n","    updated_params.append([w, b])\n","  return updated_params"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-x6AcKKrTThM","colab_type":"code","colab":{}},"source":["x = torch.randint(0, 10, [1, 3], device=device, dtype=dtype)\n","y = torch.randint(0, 10, [1, 2], device=device, dtype=dtype)\n","y[:, 0] = 0\n","y[:, 1] = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LebtJDFI4NOd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1592476038371,"user_tz":-420,"elapsed":1327,"user":{"displayName":"Van Anh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjY6DXrvLxeXSifvLvjs3m3nEAebAooQkVA9TUh=s64","userId":"08565157803240051730"}},"outputId":"a039da02-e339-4e96-d157-3b349818c727"},"source":["print(x)\n","print(y)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[8., 9., 5.]], device='cuda:0')\n","tensor([[0., 1.]], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-Ih8smLakVyE","colab_type":"text"},"source":["Init and train model"]},{"cell_type":"code","metadata":{"id":"JSVhoNOVf2v_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":505},"executionInfo":{"status":"ok","timestamp":1592477494635,"user_tz":-420,"elapsed":2546,"user":{"displayName":"Van Anh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjY6DXrvLxeXSifvLvjs3m3nEAebAooQkVA9TUh=s64","userId":"08565157803240051730"}},"outputId":"90bfc208-bb9b-4458-b2f9-775bf3746eb3"},"source":["model_params = init_model(3, layer_config=[5, 2])\n","print(\"y:\", y, \"\\n\")\n","for i in range(2000):\n","  outputs = run_model(x=x, model_params=model_params, acti_fs=['relu', 'softmax'])\n","  if i == 0:\n","    print(outputs[-1][-1])\n","  loss, loss_grad = loss_fn(outputs[-1][-1], y)\n","  if i%100 == 0:\n","    print(loss)\n","  model_params = backward(model_params, acti_fs=['relu', 'softmax'], output_layers=outputs, loss_gra=loss_grad, lr=0.07)\n","\n","print(\"\\n\", outputs[-1][-1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["y: tensor([[0., 1.]], device='cuda:0') \n","\n","tensor([[0.0082, 0.9918]], device='cuda:0')\n","0.00013357952411752194\n","9.512425458524376e-05\n","7.38535454729572e-05\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  if sys.path[0] == '':\n"],"name":"stderr"},{"output_type":"stream","text":["6.0351369029376656e-05\n","5.102079740026966e-05\n","4.418753815116361e-05\n","3.896611451637e-05\n","3.4848366340156645e-05\n","3.151717100990936e-05\n","2.876606595236808e-05\n","2.6456289560883306e-05\n","2.4490018404321745e-05\n","2.2795253244112246e-05\n","2.1319996449165046e-05\n","2.0023737306473777e-05\n","1.8875427485909313e-05\n","1.7852962628239766e-05\n","1.6934965969994664e-05\n","1.6106290786410682e-05\n","1.535378396511078e-05\n","\n"," tensor([[0.0027, 0.9973]], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8XRhGKex5T98","colab_type":"text"},"source":["##2. Pytorch code"]},{"cell_type":"code","metadata":{"id":"TlEmhvcJ-oWC","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592543358873,"user_tz":-420,"elapsed":857,"user":{"displayName":"Van Anh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjY6DXrvLxeXSifvLvjs3m3nEAebAooQkVA9TUh=s64","userId":"08565157803240051730"}}},"source":["# add requires_grad=True when init w and b\n","def init_model_grad(input_shape=1000, layer_config=[1000, 500, 10], device=\"cuda:0\", dtype=torch.float):\n","  # Init parameter of model\n","  # Return params = [[w1, b1], [w2, b2], ...]\n","  params = []\n","  i_shape = input_shape\n","  for num_n in layer_config:\n","    w = torch.randn([i_shape, num_n], device=device, dtype=dtype, requires_grad=True)\n","    b = torch.randn([1, num_n], device=device, dtype=dtype, requires_grad=True)\n","    # w = torch.randint(-10, 10, [i_shape, num_n], device=device, dtype=dtype)\n","    # b = torch.randint(-10, 10, [1, num_n], device=device, dtype=dtype)\n","    i_shape = num_n\n","\n","    params.append([w, b])\n","  return params"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"ruchkQIg5XPK","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592542585911,"user_tz":-420,"elapsed":744,"user":{"displayName":"Van Anh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjY6DXrvLxeXSifvLvjs3m3nEAebAooQkVA9TUh=s64","userId":"08565157803240051730"}}},"source":["# Init data\n","x = torch.randint(0, 10, [1, 3], device=device, dtype=dtype)\n","y = torch.randint(0, 10, [1, 2], device=device, dtype=dtype)\n","y[:, 0] = 0\n","y[:, 1] = 1"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"qcBI9-ku53f3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":436},"executionInfo":{"status":"ok","timestamp":1592543429269,"user_tz":-420,"elapsed":3342,"user":{"displayName":"Van Anh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjY6DXrvLxeXSifvLvjs3m3nEAebAooQkVA9TUh=s64","userId":"08565157803240051730"}},"outputId":"4209e806-0eee-4f0d-a2d0-99147d5ba1f2"},"source":["# Init model\n","model_params = init_model_grad(3, layer_config=[5, 2])\n","\n","lr = 0.07\n","\n","# Train model\n","for i in range(2000):\n","  output = run_model(x, model_params=model_params, acti_fs=['relu', 'softmax'])\n","  y_pred = output[-1][-1]\n","  loss = (y_pred - y).pow(2).sum()\n","  if i%100 == 0:\n","    print(\"loss\", i, \":\\t\", loss.item())\n","  \n","  # enable autograd()\n","  loss.backward()\n","  # model_params_update = model_params.clone()\n","  # update w, b\n","  with torch.no_grad():\n","    for w, b in model_params:\n","      w -= lr * w.grad\n","      b -= lr * b.grad\n","\n","      w.grad.zero_()\n","      b.grad.zero_()\n","print(y_pred)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  from ipykernel import kernelapp as app\n"],"name":"stderr"},{"output_type":"stream","text":["loss 0 :\t 0.8558268547058105\n","loss 100 :\t 0.0013833274133503437\n","loss 200 :\t 0.0005539511912502348\n","loss 300 :\t 0.0003328070160932839\n","loss 400 :\t 0.0002338044869247824\n","loss 500 :\t 0.00017850073345471174\n","loss 600 :\t 0.00014349933189805597\n","loss 700 :\t 0.00011948654719162732\n","loss 800 :\t 0.00010205530270468444\n","loss 900 :\t 8.886550494935364e-05\n","loss 1000 :\t 7.855683361412957e-05\n","loss 1100 :\t 7.02936522429809e-05\n","loss 1200 :\t 6.353146454785019e-05\n","loss 1300 :\t 5.790196155430749e-05\n","loss 1400 :\t 5.3145318815950304e-05\n","loss 1500 :\t 4.907741822535172e-05\n","loss 1600 :\t 4.556164640234783e-05\n","loss 1700 :\t 4.249424091540277e-05\n","loss 1800 :\t 3.979599568992853e-05\n","loss 1900 :\t 3.74043156625703e-05\n","tensor([[0.0042, 0.9958]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r7tD_gkT79Z5","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}