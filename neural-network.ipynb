{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"neural-network.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP7cF6/TIvBAheMKDC7cUpX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Ddrn46IBbWvJ","colab_type":"text"},"source":["#Neural network model by Pytorh  \n","Create basic NN model step by step from init model to compute gradient and train model"]},{"cell_type":"code","metadata":{"id":"zMgv9rFmn9O9","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592533876475,"user_tz":-420,"elapsed":2118,"user":{"displayName":"Van Anh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjY6DXrvLxeXSifvLvjs3m3nEAebAooQkVA9TUh=s64","userId":"08565157803240051730"}}},"source":["import torch\n","\n","device = torch.device(\"cuda:0\")\n","dtype=torch.float"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N4D3AL0Nah-X","colab_type":"text"},"source":["Check data type"]},{"cell_type":"code","metadata":{"id":"PP7GL2Guq_g_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1592533920504,"user_tz":-420,"elapsed":1364,"user":{"displayName":"Van Anh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjY6DXrvLxeXSifvLvjs3m3nEAebAooQkVA9TUh=s64","userId":"08565157803240051730"}},"outputId":"d83a35ef-826f-4d82-a8fa-975086a3806e"},"source":["a = torch.randint(0, 10, [3, 4], device=device, dtype=torch.float)\n","print(a)\n","# print(a.shape)\n","# w = torch.tensor([10, 10, 10], device=device, dtype=torch.float)\n","w = torch.randint(0, 10, [3, 4], device=device, dtype=torch.float)\n","print(\"W before\", w)\n","# print(\"W shape\", w.shape)\n","# w = w.t()\n","# print(\"W after\", w)\n","# a = a.matmul(w)\n","# print(\"a after mm\", a)\n","# print(a.shape)\n","print(a*w)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["tensor([[2., 4., 5., 4.],\n","        [7., 1., 1., 7.],\n","        [6., 5., 5., 4.]], device='cuda:0')\n","W before tensor([[4., 3., 9., 4.],\n","        [6., 3., 2., 5.],\n","        [4., 6., 2., 2.]], device='cuda:0')\n","tensor([[ 8., 12., 45., 16.],\n","        [42.,  3.,  2., 35.],\n","        [24., 30., 10.,  8.]], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iwuMd9ccLoTx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1592533941731,"user_tz":-420,"elapsed":2158,"user":{"displayName":"Van Anh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjY6DXrvLxeXSifvLvjs3m3nEAebAooQkVA9TUh=s64","userId":"08565157803240051730"}},"outputId":"6773ef12-6754-4715-804e-320bad29b272"},"source":["x = torch.tensor([1,2,3,4], device=device)\n","y = torch.tensor([3,4,5], device=device)\n","x2, y2 = torch.meshgrid(x, x)\n","print(x2 * torch.eye(4, device=device))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["tensor([[1., 0., 0., 0.],\n","        [0., 2., 0., 0.],\n","        [0., 0., 3., 0.],\n","        [0., 0., 0., 4.]], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BNjO-Vv_3H43","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592534026663,"user_tz":-420,"elapsed":1715,"user":{"displayName":"Van Anh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjY6DXrvLxeXSifvLvjs3m3nEAebAooQkVA9TUh=s64","userId":"08565157803240051730"}}},"source":["def init_model(input_shape=1000, layer_config=[1000, 500, 10], device=\"cuda:0\", dtype=torch.float):\n","  # Init parameter of model\n","  params = []\n","  i_shape = input_shape\n","  for num_n in layer_config:\n","    w = torch.randn([i_shape, num_n], device=device, dtype=dtype)\n","    b = torch.randn([1, num_n], device=device, dtype=dtype)\n","    # w = torch.randint(-10, 10, [i_shape, num_n], device=device, dtype=dtype)\n","    # b = torch.randint(-10, 10, [1, num_n], device=device, dtype=dtype)\n","    i_shape = num_n\n","\n","    params.append([w, b])\n","  return params"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"UzasxlkqqZjU","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592534028225,"user_tz":-420,"elapsed":921,"user":{"displayName":"Van Anh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjY6DXrvLxeXSifvLvjs3m3nEAebAooQkVA9TUh=s64","userId":"08565157803240051730"}}},"source":["def run_model(x=None, model_params=None, acti_fs=['relu', 'softmax']):\n","  # return output of each layer and each step in layer\n","  # output = [[input], [h1, h1_ac], [h2, h2_ac], ...]\n","  out_l = x\n","  assert len(model_params) == len(acti_fs)\n","  outputs = [[x]]\n","  for (w, b), ac in zip(model_params, acti_fs):\n","    out_l_list = []\n","    out_l = out_l.matmul(w) + b\n","    out_l_list.append(out_l)\n","    if ac is 'relu':\n","      out_l = out_l.clamp(min=0)\n","    elif ac is 'softmax':\n","      out_l = torch.nn.Softmax()(out_l)\n","    out_l_list.append(out_l)\n","    outputs.append(out_l_list)\n","  return outputs\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"vESzuDGwUvlW","colab_type":"code","colab":{}},"source":["def loss_fn(y_pred, y_gt):\n","  # compute loss and loss gradient\n","  loss = (y_pred - y_gt).pow(2).sum().item()\n","  loss_grad = 2.0 * (y_pred - y_gt)\n","  return loss, loss_grad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2TCdaDrEUUzC","colab_type":"code","colab":{}},"source":["def backward(model_params=None, acti_fs=['relu', 'softmax'], output_layers=[], lr=0.001, loss_gra=[0.0], device='cuda:0', dtype=torch.float):\n","  ## compute gradient of each layer and update weight, bias\n","  debug = False\n","  grads = []\n","  grad_previous_layer = loss_gra\n","  if debug:\n","    print(\"Step 0\")\n","    print(\"\\tgrad_previous_layer\", grad_previous_layer.shape)\n","  # compute gradient from the last layer to first layer\n","  for idx in range(len(model_params))[::-1]:\n","    w, b = model_params[idx] # w, b of layer\n","    ac =  acti_fs[idx]        # activation func of layer\n","    out_l = output_layers[idx + 1]  # output of layer (because output[0] is input), out_l = [h, h_ac]\n","    out_pre = output_layers[idx]    # output of previous layer (because output[0] is input)\n","\n","    # out_l[0] = w * out_pre[-1] + b\n","    # out_l[1] = ac(out_l[0])\n","    if debug:\n","      print(\"Step\", idx)\n","    if ac == 'softmax':\n","      grad_softmax = out_l[-1] * (1.0 - out_l[-1] + grad_previous_layer / 2.0) # 1x2\n","      grad_softmax = grad_softmax.t().matmul(torch.ones([1, grad_softmax.shape[-1]], device=device, dtype=dtype))\n","      eye_matrix = torch.eye(grad_softmax.shape[-1], device=device, dtype=dtype)\n","      grad_softmax = grad_softmax * eye_matrix  # = 0 for i#j\n","      grad_previous_layer = grad_previous_layer.matmul(grad_softmax)\n","\n","      if debug:\n","        print(\"\\tgrad_softmax\", grad_softmax.shape)\n","        print(\"\\tgrad_previous_layer\", grad_previous_layer.shape)\n","    elif ac == 'relu':\n","      grad_previous_layer[out_l[0] < 0] = 0\n","      if debug:\n","        print(\"\\tgrad_relu/grad_previous_layer\", grad_previous_layer.shape)\n","    if debug:\n","      print(\"\\tout_pre\", out_pre[-1].shape)\n","\n","    grad_w = out_pre[-1].t().mm(grad_previous_layer)\n","    grad_b = grad_previous_layer.clone()\n","    grads.append([grad_w, grad_b])\n","\n","    if debug:\n","      print(\"\\tgrad_w\", grad_w.shape)\n","      print(\"\\tgrad_b\", grad_b.shape)\n","      print(\"\\tw\", w.shape)\n","\n","    grad_previous_layer = grad_previous_layer.mm(w.t())\n","    if debug:\n","      print(\"\\tgrad_previous_layer\", grad_previous_layer.shape)\n","\n","  grads = grads[::-1]\n","  # update weight and bias\n","  if debug:\n","    print(len(model_params), len(grads))\n","  updated_params = []\n","  for (w, b), (grad_w, grad_b) in zip(model_params, grads):\n","    w -= lr * grad_w\n","    b -= lr * grad_b\n","    # print(\"\\tgrad_w\", grad_w)\n","    updated_params.append([w, b])\n","  return updated_params"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-x6AcKKrTThM","colab_type":"code","colab":{}},"source":["x = torch.randint(0, 10, [1, 3], device=device, dtype=dtype)\n","y = torch.randint(0, 10, [1, 2], device=device, dtype=dtype)\n","y[:, 0] = 0\n","y[:, 1] = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LebtJDFI4NOd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1592476038371,"user_tz":-420,"elapsed":1327,"user":{"displayName":"Van Anh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjY6DXrvLxeXSifvLvjs3m3nEAebAooQkVA9TUh=s64","userId":"08565157803240051730"}},"outputId":"a039da02-e339-4e96-d157-3b349818c727"},"source":["print(x)\n","print(y)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[8., 9., 5.]], device='cuda:0')\n","tensor([[0., 1.]], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-Ih8smLakVyE","colab_type":"text"},"source":["Init and train model"]},{"cell_type":"code","metadata":{"id":"JSVhoNOVf2v_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":505},"executionInfo":{"status":"ok","timestamp":1592477494635,"user_tz":-420,"elapsed":2546,"user":{"displayName":"Van Anh Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjY6DXrvLxeXSifvLvjs3m3nEAebAooQkVA9TUh=s64","userId":"08565157803240051730"}},"outputId":"90bfc208-bb9b-4458-b2f9-775bf3746eb3"},"source":["model_params = init_model(3, layer_config=[5, 2])\n","print(\"y:\", y, \"\\n\")\n","for i in range(2000):\n","  outputs = run_model(x=x, model_params=model_params, acti_fs=['relu', 'softmax'])\n","  if i == 0:\n","    print(outputs[-1][-1])\n","  loss, loss_grad = loss_fn(outputs[-1][-1], y)\n","  if i%100 == 0:\n","    print(loss)\n","  model_params = backward(model_params, acti_fs=['relu', 'softmax'], output_layers=outputs, loss_gra=loss_grad, lr=0.07)\n","\n","print(\"\\n\", outputs[-1][-1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["y: tensor([[0., 1.]], device='cuda:0') \n","\n","tensor([[0.0082, 0.9918]], device='cuda:0')\n","0.00013357952411752194\n","9.512425458524376e-05\n","7.38535454729572e-05\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  if sys.path[0] == '':\n"],"name":"stderr"},{"output_type":"stream","text":["6.0351369029376656e-05\n","5.102079740026966e-05\n","4.418753815116361e-05\n","3.896611451637e-05\n","3.4848366340156645e-05\n","3.151717100990936e-05\n","2.876606595236808e-05\n","2.6456289560883306e-05\n","2.4490018404321745e-05\n","2.2795253244112246e-05\n","2.1319996449165046e-05\n","2.0023737306473777e-05\n","1.8875427485909313e-05\n","1.7852962628239766e-05\n","1.6934965969994664e-05\n","1.6106290786410682e-05\n","1.535378396511078e-05\n","\n"," tensor([[0.0027, 0.9973]], device='cuda:0')\n"],"name":"stdout"}]}]}